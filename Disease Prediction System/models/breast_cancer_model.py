# -*- coding: utf-8 -*-
"""Breast_Cancer_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IYJIQ-Jh1FCZwEzgUHJPrmMltJeiINFO

# Data Loading and Initial Exploration

This section focuses on loading the breast cancer dataset and performing initial data exploration to understand its structure and characteristics.
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score
import pickle
import warnings
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings("ignore")

"""## **Data Loading**
### Loading the Breast Cancer Wisconsin Diagnostic dataset using pandas.
"""

# Loading dataset
df = pd.read_csv("breast_cancer_wisconsin_diagnostic.csv")

df.head()

df.info()

df.isnull().sum()

"""## **Data Cleaning and Preprocessing**
### Handling missing values, outliers, and duplicate data to ensure data quality.
### Converting categorical features into numerical representation for model compatibility.
"""

df.drop(columns=['Unnamed: 0'], inplace=True)

plt.figure(figsize=(40, 18))
sns.boxplot(df)
plt.title('Box Plot of feature')
plt.show()

# Checking the outlier counts
def outlier_count(df, col):
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    return len(outliers)

cols = ['perimeter1', 'area1', 'area2', 'texture3', 'perimeter3', 'area3']
for col in cols:
    outlier_num = outlier_count(df, col)
    print(f"Number of outliers in {col}: {outlier_num}")

#Capping the outliers to lower or upper bound
def cap_outliers(df, col):
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
    df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    return df

cols = ['perimeter1', 'area1', 'area2', 'texture3', 'perimeter3', 'area3']
for col in cols:
    df = cap_outliers(df, col)

#Check for duplicates
duplicate_rows = df[df.duplicated()]
print(f"Number of duplicate rows: {len(duplicate_rows)}")
if len(duplicate_rows)>0:
  print("Duplicate Rows :")
  print(duplicate_rows)
  #Remove duplicates
  df.drop_duplicates(inplace=True)
  print(f"Number of rows after removing duplicates: {len(df)}")

df.Diagnosis.value_counts()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Diagnosis'] = le.fit_transform(df['Diagnosis'])

"""## **Exploratory Data Analysis (EDA)**
### Visualizing data distributions and relationships between features to gain insights.
### Checking for correlations between features and the target variable.
"""

plt.figure(figsize=(30, 18))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

# Separating features and labels
X = df.drop(columns=['Diagnosis'])  # Drop target variable
y = df['Diagnosis']

"""## **Feature Engineering and Selection**
### Selecting relevant features using Recursive Feature Elimination (RFE) to improve model performance.
"""

# #Feature Selection using Recursive Feature Elimination (RFE)
# rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=10, step=1)
# rfe_selector.fit(X, y)

# selected_features = X.columns[rfe_selector.support_]
# print("Selected Features:", selected_features)

from xgboost import XGBClassifier
from sklearn.feature_selection import SelectFromModel

from xgboost import XGBClassifier
import pandas as pd
from sklearn.feature_selection import SelectFromModel

# Train the XGBoost classifier
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
xgb.fit(X, y)

# Get feature importances as a Series
feature_importances = pd.Series(xgb.feature_importances_, index=X.columns)

# Select top 10 features
top_10_features = feature_importances.sort_values(ascending=False).head(10).index.tolist()
print("Top 10 Selected Features by XGBoost:", top_10_features)

# Subset the data
X_selected = X[top_10_features]

"""## **Model Training and Evaluation**
### Training and evaluating different machine learning models to identify the best-performing one.
### Using cross-validation to assess model generalization ability.
"""

# Splitting data into training/testing sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.25, random_state=42)

# Initialize models with class_weight='balanced'
models = {
    "Logistic Regression": LogisticRegression(class_weight='balanced'),
    "Random Forest": RandomForestClassifier(class_weight='balanced', random_state=42),
    "Decision Tree": DecisionTreeClassifier(class_weight='balanced', random_state=42),
    "SVC": SVC(class_weight='balanced', probability=True, random_state=42)
}

from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score

# Train and evaluate each model
results = {}
for model_name, model in models.items():
    print(f"Training {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[model_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    }
    print(f"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")
    print("-" * 30)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))

for name, model in models.items():
    # Get probability scores (needed for ROC curve)
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
    elif hasattr(model, "decision_function"): # For models like SVC
        y_proba = model.decision_function(X_test)
    else:
        # Skip models that don't provide probabilities or decision functions
        print(f"Skipping ROC curve for {name} (no predict_proba or decision_function)")
        continue

    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random') # Random guess line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.grid(True)
plt.show()

"""What ROC Curve Shows:
The ROC (Receiver Operating Characteristic) curve plots the True Positive Rate against the False Positive Rate.

A model that perfectly classifies all positives and negatives will have an AUC (Area Under Curve) of 1.0.

The closer the curve is to the top-left corner, the better the model.

The AUC score quantifies overall performance.

## **Model Deployment**
### Saving the best-performing model for future use.
"""

# Saving the Logistic Regression model
best_model = models["Logistic Regression"]

# Create a pickle file for the model
filename = 'logistic_regression_Breast_Cancer_model.pkl'
pickle.dump(best_model, open(filename, 'wb'))
print(f"Logistic Regression model saved to {filename}")

