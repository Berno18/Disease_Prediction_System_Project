# -*- coding: utf-8 -*-
"""Diabetes_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lldbf92z8dEIMC0L2y60T9rH1hpHEI-E

# 1.Importing Libraries

Imports the pandas library for data manipulation and numpy for numerical operations. These are fundamental libraries for data science in Python.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# 2. Loading Data

Reads a CSV file named "diabetes_prediction_dataset.csv" into a pandas DataFrame called df. This loads the dataset for analysis.
"""

df = pd.read_csv("diabetes_prediction_dataset.csv")

"""# 3. Data Exploration

These lines display the first few rows of the DataFrame (df.head()), provide information about the DataFrame's structure and data types (df.info()), and generate descriptive statistics of the numerical columns (df.describe()). These are common steps for initial data exploration.
"""

df.head()

df.info()

df.describe()

"""## 4. Data Cleaning and Preprocessing
### Handling missing values, outliers, and duplicate data to ensure data quality.
### Converting categorical features into numerical representation for model compatibility.
"""

plt.figure(figsize=(40, 18))
sns.boxplot(df)
plt.title('Box Plot of feature')
plt.show()

# Checking the outlier counts
def outlier_count(df, col):
    q1 = df[col].quantile(0.25)
    q3 = df[col].quantile(0.75)
    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    return len(outliers)

cols = ['bmi', 'HbA1c_level', 'hypertension', 'heart_disease', 'blood_glucose_level']
for col in cols:
    outlier_num = outlier_count(df, col)
    print(f"Number of outliers in {col}: {outlier_num}")

"""# 5. Class Distribution"""

df.diabetes.value_counts()

df.shape

df.isnull().sum()

"""# 6. Data Preprocessing

Imports the LabelEncoder class from sklearn.preprocessing to convert categorical features ('smoking_history' and 'gender') into numerical representations. This is often necessary for machine learning algorithms that work with numerical data.
"""

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

df["smoking_history"]=le.fit_transform(df["smoking_history"])

df["gender"]=le.fit_transform(df["gender"])

df.info()

df.head()

"""## 7. Exploratory Data Analysis (EDA)
### Visualizing data distributions and relationships between features to gain insights.
### Checking for correlations between features and the target variable.
"""

plt.figure(figsize=(30, 18))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap')
plt.show()

"""# 8. Feature and Target Separation

Separates the dataset into features (X) and the target variable (y). X contains all columns except 'diabetes', and y contains the 'diabetes' column, which is the variable we want to predict.
"""

X=df.drop('diabetes',axis=1)
y=df['diabetes']

"""# 9. Handling Class Imbalance

Imports the SMOTE class from the imblearn.over_sampling module, which is used for oversampling the minority class in an imbalanced dataset. It creates synthetic samples to balance the class distribution. This step is performed because, according to the Counter, the data contains significantly more examples of one class compared to the other.
"""

from imblearn.over_sampling import SMOTE

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X, y = smote.fit_resample(X, y)

"""# 10. Model Training and Evaluation

This section imports necessary libraries for model selection, training, and evaluation. It splits the data into training and testing sets using train_test_split. Then, it initializes and trains four different classification models (Logistic Regression, Random Forest, Decision Tree, and SVC) and evaluates their performance using accuracy score.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Suppress warnings
import warnings
warnings.filterwarnings("ignore")

# Initialize and train the logistic regression model
model_lr = LogisticRegression()
model_lr.fit(X_train, y_train)

y_pred = model_lr.predict(X_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Logistic Regression model: {accuracy}")

# Initialize and train the Random Forest Classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Random Forest model: {accuracy}")

# Initialize and train the Decision Tree Classifier
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

y_pred = dt_model.predict(X_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Decision Tree model: {accuracy}")

# Initialize and train the SVC model
svc_model = SVC(random_state=42)
svc_model.fit(X_train, y_train)

y_pred = svc_model.predict(X_test)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the SVC model: {accuracy}")

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "SVC": SVC(random_state=42)
}

from sklearn.metrics import recall_score, f1_score, precision_score, accuracy_score

# Train and evaluate each model
results = {}
for model_name, model in models.items():
    print(f"Training {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[model_name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    }
    print(f"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")
    print("-" * 30)

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))

for name, model in models.items():
    # Get probability scores (needed for ROC curve)
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test)[:, 1]
    elif hasattr(model, "decision_function"): # For models like SVC
        y_proba = model.decision_function(X_test)
    else:
        # Skip models that don't provide probabilities or decision functions
        print(f"Skipping ROC curve for {name} (no predict_proba or decision_function)")
        continue

    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Random') # Random guess line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.grid(True)
plt.show()

"""# 11. Saving the Model

: Imports the pickle library and saves the trained Random Forest model to a file named 'randomforestmodel_diabetes.pkl'. This allows you to load and reuse the model later without retraining.
"""

import pickle

# Save the Random Forest model
filename = 'randomforestmodel_diabetes.pkl'
pickle.dump(rf_model, open(filename, 'wb'))

print(f"Random Forest model saved as {filename}")

